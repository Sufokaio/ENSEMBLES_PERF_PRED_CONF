# Improving Configurable Software Performance Prediction via Homogeneous Ensembles and Multi-Metric Evaluation

This repository contains the source code, the data used, and the raw
results for each experiment from the corresponding paper.

## Documents

### /data

This folder contains the data used for all experiments. There is one CSV
file for each subject system (eight in total). Each column represents a
configuration option, and the last column is the performance value to be
predicted.

### /results_paper
Due to the large size of the datasets and raw results, separate download links are provided for each dataset in `results_paper/results_paper.txt`. The structure of the folders is explained below.
Later executions will save results in a separate `/results` folder.

The format of the results is as follows:

One folder per dataset, and within each dataset folder, one folder per
sample size (five per dataset).

In each sample size subfolder, there are eight files named
`{Model-Name}_metrics_results.json`. These files represent the results
of the single variants of the corresponding model type (eight model
types).

Each file has the following format:

A JSON file containing one list with ten dict entries. Each dict
represents one variant of the corresponding model with different
hyperparameters (visible in the `"Params"` entry). Each dict contains
the original `"Rank"` entry from the previous hyperparameter tuning, the
new `"Borda_Rank"` entry based on our evaluation method, and the mean
values across 30 runs for each evaluation metric (`Mean_MAE`,
`Mean_MRE`, `Mean_MBRE`, `Mean_MIBRE`). Additionally, `"Metrics"` and
`"Runs"` contain the evaluation metric values of each run and the raw
predictions of each run for each element in the corresponding test sets.
Each entry includes its SA comparison to assess whether the selected
learners perform meaningfully better than chance.

Our experiments include 27 different combinations of ensemble variants
for each learning technique: 3 combination rules (Mean, IRWM, and
NN) and 9 different numbers of single learners (from 2 to 10).
Similar to the single variants, we have:

-   `{Model-Name}_top{k}_predictions.json` for the Mean combination rule
-   `{Model-Name}_top{k}_irwm_predictions.json` for the IRWM combination
    rule
-   `{Model-Name}_top{k}_nn_predictions.json` for the NN combination
    rule

with `k` representing the number of single learners.

These files contain a single dict with the mean values across 30 runs
for each evaluation metric (`Mean_MAE`, `Mean_MRE`, `Mean_MBRE`,
`Mean_MIBRE`). `"Metrics"` and `"Runs"` contain the evaluation metric
values of each run and the raw predictions of each run for each element
in the corresponding test sets.

### /DEEPPERF and /HINNPERF

These directories contain the implementations of both state-of-the-art
approaches, using their publicly available resources:
https://github.com/DeepPerf/DeepPerf and https://drive.google.com/drive/folders/1qxYzd5Om0HE1rK0syYQsTPhTQEBjghLh

### ML Techniques

Each ML technique has its own class based on the `Base` class in
`base.py`. It contains the general logic described in the paper for
constructing the single variants.

### main.py

This file contains the general logic described in the paper for
evaluating the single variants from the previous step and constructing
and evaluating the ensemble variants.

## Prerequisites and Installation

Use `requirements.txt` and follow any runtime messages. The experiments were executed with Python 3.9.x.

## How to Run

Comment out the datasets and models you want to exclude. Otherwise, it
will run all experiments by default, as described in the paper.

To reproduce the experiments from the paper:

    python main.py --model baseline
    python main.py

Note: Results from the next execution will be saved in `/results` by default.
